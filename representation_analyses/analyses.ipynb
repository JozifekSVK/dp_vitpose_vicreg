{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initializing google colab enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtJGNm42Gjc0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "! git clone https://github.com/JozifekSVK/dp_vitpose_vicreg.git\n",
        "! cd dp_vitpose_vicreg/mmcv && python setup.py install\n",
        "! pip install -v -e dp_vitpose_vicreg/ViTPose/.\n",
        "! pip install timm einops\n",
        "! pip install yapf==0.40.1\n",
        "! pip install umap-learn\n",
        "! pip install pycocotools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spM8ii6uRFzK"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import requests\n",
        "import umap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from PIL import Image\n",
        "from pycocotools.coco import COCO\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "import collections\n",
        "from dp_vitpose_vicreg.ijepa.src.models.vision_transformer import vit_small ### Defining model\n",
        "\n",
        "IMAGES_IN_CLASS = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IOlcfgJHyxw"
      },
      "outputs": [],
      "source": [
        "! unzip \"/content/drive/MyDrive/DP_pose_estimation/Dataset/COCO_dataset/Dataset_dp.zip\" -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmUNtnm2GyaV"
      },
      "outputs": [],
      "source": [
        "def make_transforms(\n",
        "    crop_size=224,\n",
        "    crop_scale=(0.3, 1.0),\n",
        "    normalization=((0.485, 0.456, 0.406),\n",
        "                   (0.229, 0.224, 0.225))\n",
        "):\n",
        "\n",
        "    transform_list = []\n",
        "    transform_list += [transforms.RandomResizedCrop(crop_size, scale=crop_scale)]\n",
        "    transform_list += [transforms.ToTensor()]\n",
        "    transform_list += [transforms.Normalize(normalization[0], normalization[1])]\n",
        "\n",
        "    transform = transforms.Compose(transform_list)\n",
        "    return transform\n",
        "\n",
        "def get_links_from_category(category_name, coco):\n",
        "  catIds = coco.getCatIds(catNms=[category_name])\n",
        "  imgIds = coco.getImgIds(catIds=catIds)\n",
        "  images = coco.loadImgs(imgIds)\n",
        "\n",
        "  return images\n",
        "\n",
        "def download_five_images(links):\n",
        "  result = []\n",
        "  for i in range(IMAGES_IN_CLASS):\n",
        "\n",
        "    session = requests.Session()\n",
        "    retry = Retry(connect=3, backoff_factor=0.5)\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    session.mount('http://', adapter)\n",
        "    session.mount('https://', adapter)\n",
        "\n",
        "    img_data = session.get(links[i]['coco_url']).content\n",
        "\n",
        "    result.append(img_data)\n",
        "\n",
        "  return result\n",
        "\n",
        "def print_images(images):\n",
        "  converted_images = []\n",
        "  for image in images:\n",
        "    nparr = np.frombuffer(image, np.uint8)\n",
        "    img = cv2.imdecode(nparr,cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    converted_images.append(img)\n",
        "  \n",
        "  return converted_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initializing MS COCO dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3TywEoKyPVM",
        "outputId": "c9c1507c-885b-4322-b9a3-e9e7a8d1bb7f"
      },
      "outputs": [],
      "source": [
        "coco = COCO('/content/Dataset_dp/annotations/instances_train2017.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Downloading images from every definet class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCoaYnz_yPXR"
      },
      "outputs": [],
      "source": [
        "categories = [\n",
        "  \"person\", \"bus\", \"boat\", \"dog\", \"tennis racket\", \"banana\", \"pizza\", \"cow\", \"stop sign\", \"snowboard\"\n",
        "\n",
        "]\n",
        "\n",
        "images = {}\n",
        "for categ in categories:\n",
        "  links = get_links_from_category(categ, coco)\n",
        "  downloaded_images = download_five_images(links)\n",
        "  images[categ] = print_images(downloaded_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLscLNQ2PNNA"
      },
      "source": [
        "# I-JEPA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_6VfgotyPZb",
        "outputId": "f168f9ef-ca41-4b3b-abdc-db3d7ee72de8"
      },
      "outputs": [],
      "source": [
        "ijepa_model = \"/content/drive/MyDrive/DP_pose_estimation/pretrained_encoders/vit_small_ijepa.pth\"\n",
        "ijepa = vit_small()\n",
        "loaded_model = torch.load(ijepa_model,map_location=torch.device('cpu'))\n",
        "\n",
        "new_state_dict = collections.OrderedDict()\n",
        "for k, v in loaded_model.items():\n",
        "    name = k.replace(\"module.\", '')\n",
        "\n",
        "    if k == 'norm.weight':\n",
        "      name = 'fc_norm.weight'\n",
        "    elif k == 'norm.bias':\n",
        "      name = 'fc_norm.bias'\n",
        "\n",
        "    new_state_dict[name] = v\n",
        "\n",
        "ijepa.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE0vwYSsPSFK"
      },
      "source": [
        "# VICReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFNRPLKPyR3k",
        "outputId": "e949da98-1553-4bcc-e667-3728e3fbbfce"
      },
      "outputs": [],
      "source": [
        "vicreg_model = \"/content/drive/MyDrive/DP_pose_estimation/pretrained_encoders/vit_small_vicreg.pth\"\n",
        "vicreg = vit_small()\n",
        "loaded_model = torch.load(vicreg_model,map_location=torch.device('cpu'))\n",
        "\n",
        "new_state_dict = collections.OrderedDict()\n",
        "for k, v in loaded_model.items():\n",
        "    name = k.replace(\"module.\", '')\n",
        "\n",
        "    if k == 'module.norm.weight':\n",
        "      name = 'fc_norm.weight'\n",
        "    elif k == 'module.norm.bias':\n",
        "      name = 'fc_norm.bias'\n",
        "\n",
        "    new_state_dict[name] = v\n",
        "\n",
        "# loaded_model.fc_norm\n",
        "vicreg.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4T8zBySGFFu"
      },
      "source": [
        "# MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yC5yDs5GDrq",
        "outputId": "fd784897-14dc-4e5c-a585-5c72b88e0b59"
      },
      "outputs": [],
      "source": [
        "mae_model = \"/content/drive/MyDrive/DP_pose_estimation/pretrained_encoders/mae_backbone_trained.pth\"\n",
        "mae = vit_small()\n",
        "loaded_mae_model = torch.load(mae_model,map_location=torch.device('cpu'))\n",
        "\n",
        "new_state_dict = collections.OrderedDict()\n",
        "for k, v in loaded_mae_model.items():\n",
        "\n",
        "  if 'decoder' in k:\n",
        "    continue\n",
        "\n",
        "  if 'mask_token' in k:\n",
        "    continue\n",
        "\n",
        "  name = k.replace(\"module.\", '')\n",
        "\n",
        "  if k == 'module.norm.weight':\n",
        "    name = 'fc_norm.weight'\n",
        "  elif k == 'module.norm.bias':\n",
        "    name = 'fc_norm.bias'\n",
        "\n",
        "  new_state_dict[name] = v\n",
        "\n",
        "\n",
        "mae.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpMqeE1bWWz1"
      },
      "source": [
        "# I-JEPA-VICREG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5CtMIpIWXCO",
        "outputId": "34938941-4937-4bd0-b31d-1871ac82c4b5"
      },
      "outputs": [],
      "source": [
        "vicreg_ijepa_model = \"/content/drive/MyDrive/DP_pose_estimation/pretrained_encoders/vit_small_ijepa_vicreg.pth\"\n",
        "vicreg_ijepa = vit_small()\n",
        "loaded_vicreg_ijepa_model = torch.load(vicreg_ijepa_model,map_location=torch.device('cpu'))\n",
        "\n",
        "new_state_dict = collections.OrderedDict()\n",
        "for k, v in loaded_vicreg_ijepa_model.items():\n",
        "\n",
        "  if 'decoder' in k:\n",
        "    continue\n",
        "\n",
        "  if 'mask_token' in k:\n",
        "    continue\n",
        "\n",
        "  name = k.replace(\"module.\", '') # remove `module.`\n",
        "\n",
        "  if k == 'norm.weight':\n",
        "    name = 'fc_norm.weight'\n",
        "  elif k == 'norm.bias':\n",
        "    name = 'fc_norm.bias'\n",
        "\n",
        "  new_state_dict[name] = v\n",
        "\n",
        "\n",
        "vicreg_ijepa.load_state_dict(new_state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UMAP\n",
        "\n",
        "Defining fuctions for UMAP projection. Creating UMAP vizialization for representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIX5M1TUyR5v"
      },
      "outputs": [],
      "source": [
        "def make_predictions_and_umap(selected_model):\n",
        "    trans = make_transforms()\n",
        "    result_umap = pd.DataFrame()\n",
        "    calculated_embeddings = torch.empty(0, 196, 384)\n",
        "    labels = []\n",
        "    flatten_data = torch.empty(0, 75264)\n",
        "    for categ in categories:\n",
        "\n",
        "      images_in_category = []\n",
        "      for i in range(IMAGES_IN_CLASS):\n",
        "        PIL_image = Image.fromarray(images[categ][i])\n",
        "\n",
        "        if 3 not in images[categ][i].shape: ### do not process gray scale images\n",
        "          continue\n",
        "\n",
        "        img = trans(PIL_image)\n",
        "        images_in_category.append(img)\n",
        "        labels.append(categ)\n",
        "\n",
        "\n",
        "      input = torch.stack(images_in_category)\n",
        "      output = selected_model(input).detach()\n",
        "\n",
        "      output_flatten = torch.flatten( output,start_dim=1)\n",
        "      flatten_data = torch.cat((flatten_data,output_flatten),axis=0)\n",
        "\n",
        "      if categ == 'person':\n",
        "        output_patches_mean = output.mean(axis=1)\n",
        "        output_patches_std = output.std(axis=1)\n",
        "\n",
        "        print('Percentage of varience per sample')\n",
        "        print( (output_patches_std[0,:] / output_patches_mean[0,:].abs()).mean() )\n",
        "\n",
        "      calculated_embeddings = torch.cat((calculated_embeddings,output),axis=0)\n",
        "\n",
        "    reducer = umap.UMAP()\n",
        "\n",
        "    result_umap = reducer.fit_transform(flatten_data)\n",
        "    result_umap = pd.DataFrame(result_umap)\n",
        "    result_umap['category'] = labels\n",
        "\n",
        "    return result_umap, calculated_embeddings\n",
        "\n",
        "def create_map_plot(result_umap, model_name):  \n",
        "  colors = ListedColormap(['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])\n",
        "\n",
        "  scatter = plt.scatter(\n",
        "      result_umap[0],\n",
        "      result_umap[1],\n",
        "      c=[[x] for x in result_umap.category.map(\n",
        "          {\n",
        "              \"person\":0,\n",
        "              \"bus\":1,\n",
        "              \"boat\":2,\n",
        "              \"dog\":3,\n",
        "              \"tennis racket\":4,\n",
        "              \"banana\":5,\n",
        "              \"pizza\":6,\n",
        "              \"cow\":7,\n",
        "              \"stop sign\":8,\n",
        "              \"snowboard\":9\n",
        "          })\n",
        "      ], cmap=colors\n",
        "      )\n",
        "  plt.gca().set_aspect('equal', 'datalim')\n",
        "  plt.legend(*scatter.legend_elements())\n",
        "\n",
        "  plt.legend(handles=scatter.legend_elements()[0], labels=categories)\n",
        "  plt.title(f'UMAP projection of the embedded COCO dataset {model_name}', fontsize=24)\n",
        "  plt.show()\n",
        "\n",
        "result_umap_vicreg, embeds_vicreg = make_predictions_and_umap(vicreg)\n",
        "result_umap_ijepa, embeds_ijepa = make_predictions_and_umap(ijepa)\n",
        "result_umap_vicreg_ijepa, embeds_vicreg_ijepa = make_predictions_and_umap(vicreg_ijepa)\n",
        "result_umap_mae, embeds_mae = make_predictions_and_umap(mae)\n",
        "\n",
        "create_map_plot(result_umap_vicreg, \"VICReg\")\n",
        "create_map_plot(result_umap_ijepa, \"IJEPA\")\n",
        "create_map_plot(result_umap_vicreg_ijepa, \"IJEPA VICREG\")\n",
        "create_map_plot(result_umap_mae, \"MAE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMMoUyiRQfM4"
      },
      "source": [
        "# Distance matrix\n",
        "\n",
        "Defining function for calculating distance matrices and vizualizing them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9KWOrujqP-yT",
        "outputId": "530dc972-e30a-4c07-cdf3-860acc79ca85"
      },
      "outputs": [],
      "source": [
        "def print_dist_matrix(dist_matrix, model_name, min_value=None, max_value=None):\n",
        "  dist_matrix = np.log(dist_matrix + 1)\n",
        "  arr_median = dist_matrix.median().item()\n",
        "\n",
        "  print(f\"Median of distance matrix - {arr_median}\")\n",
        "  if min_value is not None:\n",
        "    map = plt.pcolor(dist_matrix.detach(), cmap='autumn',vmin=min_value, vmax=max_value)\n",
        "  else:\n",
        "    map = plt.pcolor(dist_matrix.detach(), cmap='autumn')\n",
        "\n",
        "  plt.colorbar(map, orientation='vertical')\n",
        "  plt.title(f\"Heatmap for embeddings from {model_name}\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def calc_dist_matrix( embeddings):\n",
        "  embeddings_copy = embeddings.clone()\n",
        "  embeddings_copy = torch.flatten( embeddings_copy,start_dim=1)\n",
        "  dist_matrix = torch.cdist(embeddings_copy, embeddings_copy, p=2)\n",
        "\n",
        "  return dist_matrix\n",
        "\n",
        "### Calculating distance matrices\n",
        "dist_matrix_vicreg = calc_dist_matrix( embeds_vicreg)\n",
        "dist_matrix_ijepa = calc_dist_matrix( embeds_ijepa)\n",
        "dist_matrix_vicreg_ijepa = calc_dist_matrix( embeds_vicreg_ijepa)\n",
        "dist_matrix_mae = calc_dist_matrix(embeds_mae)\n",
        "\n",
        "### Calculating min-max values for scaling\n",
        "concat_distance_matrices = torch.cat((dist_matrix_vicreg,dist_matrix_ijepa, dist_matrix_vicreg_ijepa, dist_matrix_mae),axis=0)\n",
        "min_value = np.log(concat_distance_matrices.min() + 1)\n",
        "max_value = np.log(concat_distance_matrices.max() + 1)\n",
        "\n",
        "print_dist_matrix(dist_matrix_vicreg, \"vicreg\", min_value, max_value)\n",
        "print_dist_matrix(dist_matrix_ijepa, \"ijepa\", min_value, max_value)\n",
        "print_dist_matrix(dist_matrix_vicreg_ijepa, \"vicreg_ijepa\", min_value, max_value)\n",
        "print_dist_matrix(dist_matrix_mae, \"mae\", min_value, max_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA projection\n",
        "\n",
        "Calculating singular values from PCA projection and visualing them into line plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "Thxgl4TmvDxL",
        "outputId": "903ced33-eefb-4627-a2b2-4557149bf6be"
      },
      "outputs": [],
      "source": [
        "### VICReg\n",
        "data_to_pca = embeds_vicreg[:,91,:]\n",
        "res = torch.pca_lowrank(data_to_pca, q=384)\n",
        "vicreg_pca = res[1]\n",
        "\n",
        "### I-JEPA\n",
        "data_to_pca = embeds_ijepa[:,91,:]\n",
        "res = torch.pca_lowrank(data_to_pca, q=384)\n",
        "ijepa_pca = res[1]\n",
        "\n",
        "### I-JEPA-VICReg\n",
        "data_to_pca = embeds_vicreg_ijepa[:,91,:]\n",
        "res = torch.pca_lowrank(data_to_pca, q=384)\n",
        "vicreg_ijepa_pca = res[1]\n",
        "\n",
        "### MAE\n",
        "data_to_pca = embeds_mae[:,91,:]\n",
        "res = torch.pca_lowrank(data_to_pca, q=384)\n",
        "mae_pca = res[1]\n",
        "\n",
        "### plot lines\n",
        "plt.plot(vicreg_pca,label='vicreg')\n",
        "plt.plot(ijepa_pca,label='ijepa')\n",
        "plt.plot(vicreg_ijepa_pca,label='vicreg_ijepa')\n",
        "plt.plot(mae_pca,label='mae')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
